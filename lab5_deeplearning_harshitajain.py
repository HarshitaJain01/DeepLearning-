# -*- coding: utf-8 -*-
"""Lab5_DeepLearning_HarshitaJain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z9G0SB2dCuZbfGRbOnwJSft4fs0hptHI

## **No Regularisation**
"""

import warnings
warnings.filterwarnings('ignore')
import numpy as np
import matplotlib.pyplot as plt
import keras
from keras.datasets import mnist
from keras.models import Sequential,model_from_json
from keras.layers import Dense
from tensorflow.keras.optimizers import RMSprop
from keras.utils import np_utils
import pylab as plt
from keras.regularizers import l1,l2

from keras.layers import Dense, Dropout

import tensorflow as tf
import tensorflow_datasets as tfds

batch_size = 128
num_classes = 10
# iteratively giving your data to your model is epochs
# How many times you are feeding data to your model
epochs = 2

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test)= mnist.load_data()
x_train = x_train.reshape(60000, 784)
x_test = x_test.reshape(10000, 784)
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

# Normalize to 0 to 1 range
x_train /= 255
x_test /= 255

print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')
# convert class vectors to binary class matrices
y_train = np_utils.to_categorical(y_train, num_classes)
y_test = np_utils.to_categorical(y_test, num_classes)
print(y_train[0])

print("Label:",y_test[2:3])
plt.imshow(x_test[2:3].reshape(28,28), cmap='gray')
plt.show()

first_layer_size = 32
model = Sequential()
model.add(Dense(first_layer_size, activation='sigmoid', input_shape=(784,)))
model.add(Dense(68, activation='sigmoid'))
model.add(Dense(68, activation='sigmoid'))
model.add(Dense(32, activation='sigmoid'))
model.add(Dense(32, activation='sigmoid'))
model.add(Dense(68, activation='sigmoid'))
model.add(Dense(68, activation='sigmoid'))
model.add(Dense(num_classes, activation='softmax'))
model.summary()

from tensorflow.keras.optimizers import Adam
model.compile(loss='categorical_crossentropy',
              optimizer=Adam(),
              metrics=['accuracy'])

history1 = model.fit(x_train,y_train,
                    batch_size=batch_size,
                    epochs=10,
                    verbose=2)
history2 = model.fit(x_test,y_test,
                    batch_size=batch_size,
                    epochs=10,
                    verbose=2)

#print(history.history.keys())
#  "Accuracy"
plt.plot(history1.history['accuracy'])
plt.plot(history2.history['accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'Validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history1.history['loss'])
plt.plot(history2.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'Validation'], loc='upper left')
plt.show()

prediction = model.predict(x_test[:1])

print("prediction shape:", prediction.shape)

model.evaluate(x_test,y_test)

first_layer_size = 32
model1 = Sequential()
model1.add(Dense(first_layer_size, activation='sigmoid', input_shape=(784,)))
model1.add(Dense(68, activation='sigmoid'))
model1.add(Dense(68, activation='sigmoid'))
model1.add(Dropout(0.5))
model1.add(Dense(32, activation='sigmoid'))
model1.add(Dense(32, activation='sigmoid'))
model1.add(Dense(68, activation='sigmoid'))
model1.add(Dense(68, activation='sigmoid'))
model1.add(Dense(num_classes, activation='softmax'))
model1.summary()

from tensorflow.keras.optimizers import Adam
model1.compile(loss='categorical_crossentropy',
              optimizer=Adam(),
              metrics=['accuracy'])
history1 = model1.fit(x_train,y_train,
                    batch_size=batch_size,
                    epochs=10,
                    verbose=2)
history2 = model1.fit(x_test,y_test,
                    batch_size=batch_size,
                    epochs=10,
                    verbose=2)

#  "Accuracy"
plt.plot(history1.history['accuracy'])
plt.plot(history2.history['accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'Validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history1.history['loss'])
plt.plot(history2.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'Validation'], loc='upper left')
plt.show()

model1.evaluate(x_test,y_test)

from tensorflow.keras import layers
from tensorflow.keras import regularizers
first_layer_size = 32
model2 = Sequential()
model2.add(Dense(first_layer_size, activation='sigmoid', input_shape=(784,)))
model2.add(Dense(68, activation='sigmoid',kernel_regularizer=regularizers.l1(0.01)))
model2.add(Dense(68, activation='sigmoid',kernel_regularizer=regularizers.l1(0.01)))
model2.add(Dense(32, activation='sigmoid',kernel_regularizer=regularizers.l1(0.01)))
model2.add(Dense(32, activation='sigmoid'))
model2.add(Dense(68, activation='sigmoid'))
model2.add(Dense(68, activation='sigmoid'))
model2.add(Dense(num_classes, activation='softmax'))
model2.summary()

from tensorflow.keras.optimizers import Adam
model2.compile(loss='categorical_crossentropy',
              optimizer=Adam(),
              metrics=['accuracy'])
history1 = model2.fit(x_train,y_train,
                    batch_size=batch_size,
                    epochs=10,
                    verbose=2)
history2 = model2.fit(x_test,y_test,
                    batch_size=batch_size,
                    epochs=10,
                    verbose=2)

#  "Accuracy"
plt.plot(history1.history['accuracy'])
plt.plot(history2.history['accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'Validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history1.history['loss'])
plt.plot(history2.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'Validation'], loc='upper left')
plt.show()

model2.evaluate(x_test,y_test)

from tensorflow.keras import layers
from tensorflow.keras import regularizers
first_layer_size = 32
model3 = Sequential()
model3.add(Dense(first_layer_size, activation='sigmoid', input_shape=(784,)))
model3.add(Dense(68, activation='sigmoid',kernel_regularizer=regularizers.l2(0.1)))
model3.add(Dense(68, activation='sigmoid',kernel_regularizer=regularizers.l2(0.1)))
model3.add(Dense(32, activation='sigmoid',kernel_regularizer=regularizers.l2(0.1)))
model3.add(Dense(32, activation='sigmoid'))
model3.add(Dense(68, activation='sigmoid'))
model3.add(Dense(68, activation='sigmoid'))
model3.add(Dense(num_classes, activation='softmax'))
model3.summary()

from tensorflow.keras.optimizers import Adam
model3.compile(loss='categorical_crossentropy',
              optimizer=Adam(),
              metrics=['accuracy'])
history1 = model3.fit(x_train,y_train,
                    batch_size=batch_size,
                    epochs=10,
                    verbose=2)
history2 = model3.fit(x_test,y_test,
                    batch_size=batch_size,
                    epochs=10,
                    verbose=2)

plt.plot(history1.history['accuracy'])
plt.plot(history2.history['accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'Validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history1.history['loss'])
plt.plot(history2.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'Validation'], loc='upper left')
plt.show()

model3.evaluate(x_test,y_test)

# example of loading the fashion mnist dataset
from matplotlib import pyplot
from keras.datasets import fashion_mnist
# load dataset
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
# summarize loaded dataset
print('Train: X=%s, y=%s' % (x_train.shape, y_train.shape))
print('Test: X=%s, y=%s' % (x_test.shape, y_test.shape))
# plot first few images
for i in range(9):
 # define subplot
 pyplot.subplot(330 + 1 + i)
 # plot raw pixel data
 pyplot.imshow(x_train[i], cmap=pyplot.get_cmap('gray'))
# show the figure
pyplot.show()

batch_size = 128
num_classes = 10
# iteratively giving your data to your model is epochs
# How many times you are feeding data to your model
epochs = 2

# the data, split between train and test sets
#(x_train, y_train), (x_test, y_test)= mnist.load_data()
x_train = x_train.reshape(60000, 784)
x_test = x_test.reshape(10000, 784)
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

# Normalize to 0 to 1 range
x_train /= 255
x_test /= 255

print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')
# convert class vectors to binary class matrices
y_train = np_utils.to_categorical(y_train, num_classes)
y_test = np_utils.to_categorical(y_test, num_classes)
print(y_train[0])

print("Label:",y_test[2:3])
plt.imshow(x_test[2:3].reshape(28,28), cmap='gray')
plt.show()

first_layer_size = 32
model = Sequential()
model.add(Dense(first_layer_size, activation='sigmoid', input_shape=(784,)))
model.add(Dense(68, activation='sigmoid'))
model.add(Dense(68, activation='sigmoid'))
model.add(Dense(32, activation='sigmoid'))
model.add(Dense(32, activation='sigmoid'))
model.add(Dense(68, activation='sigmoid'))
model.add(Dense(68, activation='sigmoid'))
model.add(Dense(num_classes, activation='softmax'))
model.summary()

from tensorflow.keras.optimizers import Adam
model.compile(loss='categorical_crossentropy',
              optimizer=Adam(),
              metrics=['accuracy'])

history1 = model.fit(x_train,y_train,
                    batch_size=batch_size,
                    epochs=10,
                    verbose=2)
history2 = model.fit(x_test,y_test,
                    batch_size=batch_size,
                    epochs=10,
                    verbose=2)

#print(history.history.keys())
#  "Accuracy"
plt.plot(history1.history['accuracy'])
plt.plot(history2.history['accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'Validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history1.history['loss'])
plt.plot(history2.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'Validation'], loc='upper left')
plt.show()

model.evaluate(x_test,y_test)

first_layer_size = 32
model1 = Sequential()
model1.add(Dense(first_layer_size, activation='sigmoid', input_shape=(784,)))
model1.add(Dense(68, activation='sigmoid'))
model1.add(Dense(68, activation='sigmoid'))
model1.add(Dropout(0.5))
model1.add(Dense(32, activation='sigmoid'))
model1.add(Dense(32, activation='sigmoid'))
model1.add(Dense(68, activation='sigmoid'))
model1.add(Dense(68, activation='sigmoid'))
model1.add(Dense(num_classes, activation='softmax'))
model1.summary()
from tensorflow.keras.optimizers import Adam
model1.compile(loss='categorical_crossentropy',
              optimizer=Adam(),
              metrics=['accuracy'])
history1 = model1.fit(x_train,y_train,
                    batch_size=batch_size,
                    epochs=10,
                    verbose=2)
history2 = model1.fit(x_test,y_test,
                    batch_size=batch_size,
                    epochs=10,
                    verbose=2)
#  "Accuracy"
plt.plot(history1.history['accuracy'])
plt.plot(history2.history['accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'Validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history1.history['loss'])
plt.plot(history2.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'Validation'], loc='upper left')
plt.show()
model1.evaluate(x_test,y_test)

from tensorflow.keras import layers
from tensorflow.keras import regularizers
first_layer_size = 32
model2 = Sequential()
model2.add(Dense(first_layer_size, activation='sigmoid', input_shape=(784,)))
model2.add(Dense(68, activation='sigmoid',kernel_regularizer=regularizers.l1(0.01)))
model2.add(Dense(68, activation='sigmoid',kernel_regularizer=regularizers.l1(0.01)))
model2.add(Dense(32, activation='sigmoid',kernel_regularizer=regularizers.l1(0.01)))
model2.add(Dense(32, activation='sigmoid'))
model2.add(Dense(68, activation='sigmoid'))
model2.add(Dense(68, activation='sigmoid'))
model2.add(Dense(num_classes, activation='softmax'))
model2.summary()
from tensorflow.keras.optimizers import Adam
model2.compile(loss='categorical_crossentropy',
              optimizer=Adam(),
              metrics=['accuracy'])
history1 = model2.fit(x_train,y_train,
                    batch_size=batch_size,
                    epochs=10,
                    verbose=2)
history2 = model2.fit(x_test,y_test,
                    batch_size=batch_size,
                    epochs=10,
                    verbose=2)
plt.plot(history1.history['accuracy'])
plt.plot(history2.history['accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'Validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history1.history['loss'])
plt.plot(history2.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'Validation'], loc='upper left')
plt.show()
model2.evaluate(x_test,y_test)

from tensorflow.keras import layers
from tensorflow.keras import regularizers
first_layer_size = 32
model3 = Sequential()
model3.add(Dense(first_layer_size, activation='sigmoid', input_shape=(784,)))
model3.add(Dense(68, activation='sigmoid',kernel_regularizer=regularizers.l2(0.1)))
model3.add(Dense(68, activation='sigmoid',kernel_regularizer=regularizers.l2(0.1)))
model3.add(Dense(32, activation='sigmoid',kernel_regularizer=regularizers.l2(0.1)))
model3.add(Dense(32, activation='sigmoid'))
model3.add(Dense(68, activation='sigmoid'))
model3.add(Dense(68, activation='sigmoid'))
model3.add(Dense(num_classes, activation='softmax'))
model3.summary()
model3.compile(loss='categorical_crossentropy',
              optimizer=Adam(),
              metrics=['accuracy'])
history1 = model3.fit(x_train,y_train,
                    batch_size=batch_size,
                    epochs=10,
                    verbose=2)
history2 = model3.fit(x_test,y_test,
                    batch_size=batch_size,
                    epochs=10,
                    verbose=2)
plt.plot(history1.history['accuracy'])
plt.plot(history2.history['accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'Validation'], loc='upper left')
plt.show()
# "Loss"
plt.plot(history1.history['loss'])
plt.plot(history2.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'Validation'], loc='upper left')
plt.show()
model3.evaluate(x_test,y_test)

